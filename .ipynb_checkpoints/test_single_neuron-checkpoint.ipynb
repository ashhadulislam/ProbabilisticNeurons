{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import pickle\n",
    "\n",
    "from scipy import stats\n",
    "    \n",
    "from torchsummary import summary\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program To train a base neuron for Pima\n",
      "Torch cuda  False\n",
      "device  cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"Program To train a base neuron for Pima\")\n",
    "\n",
    "print(\"Torch cuda \",torch.cuda.is_available())\n",
    "\n",
    "\n",
    "device = get_default_device()\n",
    "print(\"device \",device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    1   2   3    4     5      6   7  8\n",
      "0  6  148  72  35    0  33.6  0.627  50  1\n",
      "1  1   85  66  29    0  26.6  0.351  31  0\n",
      "2  8  183  64   0    0  23.3  0.672  32  1\n",
      "3  1   89  66  23   94  28.1  0.167  21  0\n",
      "4  0  137  40  35  168  43.1  2.288  33  1\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"pima-indians-diabetes.data.csv\",header=None)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.loc[:,:7].values\n",
    "y=df.loc[:,8].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y , test_size =0.2,random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Tensors\n",
    "X_train=torch.FloatTensor(X_train)\n",
    "X_test=torch.FloatTensor(X_test)\n",
    "y_train=torch.LongTensor(y_train)\n",
    "y_test=torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate(model, X_test, y_test):\n",
    "    \"\"\"Evaluate the model's performance on the validation set\"\"\"\n",
    "    outputs = [model.validation_step(X_test,y_test)]\n",
    "#     print(\"outputs are \",outputs)\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))        \n",
    "\n",
    "\n",
    "def correct(output, target, topk=(1,)):\n",
    "    \"\"\"Computes how many correct outputs with respect to targets\n",
    "\n",
    "    Does NOT compute accuracy but just a raw amount of correct\n",
    "    outputs given target labels. This is done for each value in\n",
    "    topk. A value is considered correct if target is in the topk\n",
    "    highest values of output.\n",
    "    The values returned are upperbounded by the given batch size\n",
    "\n",
    "    [description]\n",
    "\n",
    "    Arguments:\n",
    "        output {torch.Tensor} -- Output prediction of the model\n",
    "        target {torch.Tensor} -- Target labels from data\n",
    "\n",
    "    Keyword Arguments:\n",
    "        topk {iterable} -- [Iterable of values of k to consider as correct] (default: {(1,)})\n",
    "\n",
    "    Returns:\n",
    "        List(int) -- Number of correct values for each topk\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        # Only need to do topk for highest k, reuse for the rest\n",
    "        _, pred = output.topk(k=maxk, dim=1, largest=True, sorted=True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(torch.tensor(correct_k.item()))\n",
    "        return res\n",
    "\n",
    "class PIMANet(nn.Module):\n",
    "    \"\"\"Feedfoward neural network with 1 hidden layer\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(8, 1)  \n",
    "        self.fc2 = nn.Linear(1,2)\n",
    "        self.fc2.is_classifier = True\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def training_step(self, X_train, y_train):\n",
    "\n",
    "        out = self(X_train)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, y_train) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, X_test, y_test):\n",
    "        \n",
    "        out = self(X_test)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, y_test)   # Calculate loss\n",
    "        acc = accuracy(out, y_test)           # Calculate accuracy\n",
    "        top_1, top_5 = correct(out, y_test,topk=(1,2))\n",
    "#         print(\"Batch is \",batch[1].shape)\n",
    "        \n",
    "        top_1=top_1/X_test.shape[0]\n",
    "        top_5=top_5/X_test.shape[0]\n",
    "\n",
    "#         print(\"corr\",top_1,top_5)\n",
    "#         return {'val_loss': loss, 'val_acc': acc}\n",
    "        return {'val_loss': loss, 'val_acc': acc, 'top_1': top_1, 'top_5': top_5}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        \n",
    "        batch_top_1s = [x['top_1'] for x in outputs]\n",
    "#         print(batch_top_1s)\n",
    "        epoch_top_1 = torch.stack(batch_top_1s).mean()      # Combine top_1\n",
    "        \n",
    "        batch_top_5s = [x['top_5'] for x in outputs]\n",
    "        epoch_top_5 = torch.stack(batch_top_5s).mean()      # Combine top_5\n",
    "        \n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item(),\n",
    "               'val_top_1': epoch_top_1.item(), 'val_top_5': epoch_top_5.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}, val_top_1: {:.4f}, val_top_5: {:.4f}\".format(\n",
    "                                epoch, result['val_loss'], result['val_acc'], \n",
    "                                result['val_top_1'], result['val_top_5']))\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, lr, model, X_train, y_train, X_test, y_test, opt_func=torch.optim.SGD,\n",
    "               model_state_path=None):\n",
    "    \"\"\"Train the model using gradient descent\"\"\"\n",
    "    print(\"At train\")\n",
    "    history = []\n",
    "    best_so_far=-999    \n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        loss = model.training_step(X_train,y_train)\n",
    "        print(epoch,loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Validation phase\n",
    "        result = evaluate(model, X_test, y_test)\n",
    "        if best_so_far<result[\"val_top_1\"]:\n",
    "            best_so_far=result[\"val_top_1\"]\n",
    "            if model_state_path:\n",
    "                torch.save(model.state_dict(), model_state_path)\n",
    "        \n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                    [-1, 1]               9\n",
      "            Linear-2                    [-1, 2]               4\n",
      "================================================================\n",
      "Total params: 13\n",
      "Trainable params: 13\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "fc1.weight\n",
      "tensor([[ 0.0435, -0.2281,  0.2225, -0.1205, -0.1896,  0.2002,  0.2506, -0.2820]])\n",
      "fc1.bias\n",
      "tensor([-0.2208])\n",
      "fc2.weight\n",
      "tensor([[ 0.8620],\n",
      "        [-0.8201]])\n",
      "fc2.bias\n",
      "tensor([-0.3688,  0.8846])\n",
      "At train\n",
      "0 tensor(1.0919, grad_fn=<NllLossBackward>)\n",
      "Epoch [0], val_loss: 1.1123, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "1 tensor(1.0919, grad_fn=<NllLossBackward>)\n",
      "Epoch [1], val_loss: 1.1123, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "2 tensor(1.0919, grad_fn=<NllLossBackward>)\n",
      "Epoch [2], val_loss: 1.1123, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "3 tensor(1.0919, grad_fn=<NllLossBackward>)\n",
      "Epoch [3], val_loss: 1.1123, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "4 tensor(1.0919, grad_fn=<NllLossBackward>)\n",
      "Epoch [4], val_loss: 1.1123, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "5 tensor(1.0919, grad_fn=<NllLossBackward>)\n",
      "Epoch [5], val_loss: 1.1122, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "6 tensor(1.0919, grad_fn=<NllLossBackward>)\n",
      "Epoch [6], val_loss: 1.1122, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "7 tensor(1.0919, grad_fn=<NllLossBackward>)\n",
      "Epoch [7], val_loss: 1.1122, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "8 tensor(1.0919, grad_fn=<NllLossBackward>)\n",
      "Epoch [8], val_loss: 1.1122, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "9 tensor(1.0919, grad_fn=<NllLossBackward>)\n",
      "Epoch [9], val_loss: 1.1122, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "10 tensor(1.0919, grad_fn=<NllLossBackward>)\n",
      "Epoch [10], val_loss: 1.1122, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "11 tensor(1.0919, grad_fn=<NllLossBackward>)\n",
      "Epoch [11], val_loss: 1.1122, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "12 tensor(1.0919, grad_fn=<NllLossBackward>)\n",
      "Epoch [12], val_loss: 1.1122, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "13 tensor(1.0919, grad_fn=<NllLossBackward>)\n",
      "Epoch [13], val_loss: 1.1122, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "14 tensor(1.0919, grad_fn=<NllLossBackward>)\n",
      "Epoch [14], val_loss: 1.1122, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "15 tensor(1.0919, grad_fn=<NllLossBackward>)\n",
      "Epoch [15], val_loss: 1.1122, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "16 tensor(1.0918, grad_fn=<NllLossBackward>)\n",
      "Epoch [16], val_loss: 1.1122, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "17 tensor(1.0918, grad_fn=<NllLossBackward>)\n",
      "Epoch [17], val_loss: 1.1122, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "18 tensor(1.0918, grad_fn=<NllLossBackward>)\n",
      "Epoch [18], val_loss: 1.1122, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "19 tensor(1.0918, grad_fn=<NllLossBackward>)\n",
      "Epoch [19], val_loss: 1.1122, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "20 tensor(1.0918, grad_fn=<NllLossBackward>)\n",
      "Epoch [20], val_loss: 1.1122, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "21 tensor(1.0918, grad_fn=<NllLossBackward>)\n",
      "Epoch [21], val_loss: 1.1122, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "22 tensor(1.0918, grad_fn=<NllLossBackward>)\n",
      "Epoch [22], val_loss: 1.1122, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "23 tensor(1.0918, grad_fn=<NllLossBackward>)\n",
      "Epoch [23], val_loss: 1.1122, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "24 tensor(1.0918, grad_fn=<NllLossBackward>)\n",
      "Epoch [24], val_loss: 1.1122, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "25 tensor(1.0918, grad_fn=<NllLossBackward>)\n",
      "Epoch [25], val_loss: 1.1122, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "26 tensor(1.0918, grad_fn=<NllLossBackward>)\n",
      "Epoch [26], val_loss: 1.1122, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "27 tensor(1.0918, grad_fn=<NllLossBackward>)\n",
      "Epoch [27], val_loss: 1.1122, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "28 tensor(1.0918, grad_fn=<NllLossBackward>)\n",
      "Epoch [28], val_loss: 1.1122, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "29 tensor(1.0918, grad_fn=<NllLossBackward>)\n",
      "Epoch [29], val_loss: 1.1122, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "30 tensor(1.0918, grad_fn=<NllLossBackward>)\n",
      "Epoch [30], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "31 tensor(1.0918, grad_fn=<NllLossBackward>)\n",
      "Epoch [31], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "32 tensor(1.0918, grad_fn=<NllLossBackward>)\n",
      "Epoch [32], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "33 tensor(1.0917, grad_fn=<NllLossBackward>)\n",
      "Epoch [33], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "34 tensor(1.0917, grad_fn=<NllLossBackward>)\n",
      "Epoch [34], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "35 tensor(1.0917, grad_fn=<NllLossBackward>)\n",
      "Epoch [35], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "36 tensor(1.0917, grad_fn=<NllLossBackward>)\n",
      "Epoch [36], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "37 tensor(1.0917, grad_fn=<NllLossBackward>)\n",
      "Epoch [37], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "38 tensor(1.0917, grad_fn=<NllLossBackward>)\n",
      "Epoch [38], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "39 tensor(1.0917, grad_fn=<NllLossBackward>)\n",
      "Epoch [39], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "40 tensor(1.0917, grad_fn=<NllLossBackward>)\n",
      "Epoch [40], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "41 tensor(1.0917, grad_fn=<NllLossBackward>)\n",
      "Epoch [41], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "42 tensor(1.0917, grad_fn=<NllLossBackward>)\n",
      "Epoch [42], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "43 tensor(1.0917, grad_fn=<NllLossBackward>)\n",
      "Epoch [43], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "44 tensor(1.0917, grad_fn=<NllLossBackward>)\n",
      "Epoch [44], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "45 tensor(1.0917, grad_fn=<NllLossBackward>)\n",
      "Epoch [45], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "46 tensor(1.0917, grad_fn=<NllLossBackward>)\n",
      "Epoch [46], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "47 tensor(1.0917, grad_fn=<NllLossBackward>)\n",
      "Epoch [47], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "48 tensor(1.0917, grad_fn=<NllLossBackward>)\n",
      "Epoch [48], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "49 tensor(1.0916, grad_fn=<NllLossBackward>)\n",
      "Epoch [49], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "50 tensor(1.0916, grad_fn=<NllLossBackward>)\n",
      "Epoch [50], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "51 tensor(1.0916, grad_fn=<NllLossBackward>)\n",
      "Epoch [51], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "52 tensor(1.0916, grad_fn=<NllLossBackward>)\n",
      "Epoch [52], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "53 tensor(1.0916, grad_fn=<NllLossBackward>)\n",
      "Epoch [53], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "54 tensor(1.0916, grad_fn=<NllLossBackward>)\n",
      "Epoch [54], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "55 tensor(1.0916, grad_fn=<NllLossBackward>)\n",
      "Epoch [55], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "56 tensor(1.0916, grad_fn=<NllLossBackward>)\n",
      "Epoch [56], val_loss: 1.1121, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "57 tensor(1.0916, grad_fn=<NllLossBackward>)\n",
      "Epoch [57], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "58 tensor(1.0916, grad_fn=<NllLossBackward>)\n",
      "Epoch [58], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "59 tensor(1.0916, grad_fn=<NllLossBackward>)\n",
      "Epoch [59], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "60 tensor(1.0916, grad_fn=<NllLossBackward>)\n",
      "Epoch [60], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "61 tensor(1.0916, grad_fn=<NllLossBackward>)\n",
      "Epoch [61], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "62 tensor(1.0916, grad_fn=<NllLossBackward>)\n",
      "Epoch [62], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "63 tensor(1.0916, grad_fn=<NllLossBackward>)\n",
      "Epoch [63], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "64 tensor(1.0916, grad_fn=<NllLossBackward>)\n",
      "Epoch [64], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "65 tensor(1.0915, grad_fn=<NllLossBackward>)\n",
      "Epoch [65], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "66 tensor(1.0915, grad_fn=<NllLossBackward>)\n",
      "Epoch [66], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "67 tensor(1.0915, grad_fn=<NllLossBackward>)\n",
      "Epoch [67], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "68 tensor(1.0915, grad_fn=<NllLossBackward>)\n",
      "Epoch [68], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "69 tensor(1.0915, grad_fn=<NllLossBackward>)\n",
      "Epoch [69], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "70 tensor(1.0915, grad_fn=<NllLossBackward>)\n",
      "Epoch [70], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "71 tensor(1.0915, grad_fn=<NllLossBackward>)\n",
      "Epoch [71], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "72 tensor(1.0915, grad_fn=<NllLossBackward>)\n",
      "Epoch [72], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "73 tensor(1.0915, grad_fn=<NllLossBackward>)\n",
      "Epoch [73], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "74 tensor(1.0915, grad_fn=<NllLossBackward>)\n",
      "Epoch [74], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "75 tensor(1.0915, grad_fn=<NllLossBackward>)\n",
      "Epoch [75], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "76 tensor(1.0915, grad_fn=<NllLossBackward>)\n",
      "Epoch [76], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "77 tensor(1.0915, grad_fn=<NllLossBackward>)\n",
      "Epoch [77], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "78 tensor(1.0915, grad_fn=<NllLossBackward>)\n",
      "Epoch [78], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "79 tensor(1.0915, grad_fn=<NllLossBackward>)\n",
      "Epoch [79], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "80 tensor(1.0915, grad_fn=<NllLossBackward>)\n",
      "Epoch [80], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "81 tensor(1.0915, grad_fn=<NllLossBackward>)\n",
      "Epoch [81], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "82 tensor(1.0914, grad_fn=<NllLossBackward>)\n",
      "Epoch [82], val_loss: 1.1120, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "83 tensor(1.0914, grad_fn=<NllLossBackward>)\n",
      "Epoch [83], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "84 tensor(1.0914, grad_fn=<NllLossBackward>)\n",
      "Epoch [84], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "85 tensor(1.0914, grad_fn=<NllLossBackward>)\n",
      "Epoch [85], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 tensor(1.0914, grad_fn=<NllLossBackward>)\n",
      "Epoch [86], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "87 tensor(1.0914, grad_fn=<NllLossBackward>)\n",
      "Epoch [87], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "88 tensor(1.0914, grad_fn=<NllLossBackward>)\n",
      "Epoch [88], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "89 tensor(1.0914, grad_fn=<NllLossBackward>)\n",
      "Epoch [89], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "90 tensor(1.0914, grad_fn=<NllLossBackward>)\n",
      "Epoch [90], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "91 tensor(1.0914, grad_fn=<NllLossBackward>)\n",
      "Epoch [91], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "92 tensor(1.0914, grad_fn=<NllLossBackward>)\n",
      "Epoch [92], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "93 tensor(1.0914, grad_fn=<NllLossBackward>)\n",
      "Epoch [93], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "94 tensor(1.0914, grad_fn=<NllLossBackward>)\n",
      "Epoch [94], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "95 tensor(1.0914, grad_fn=<NllLossBackward>)\n",
      "Epoch [95], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "96 tensor(1.0914, grad_fn=<NllLossBackward>)\n",
      "Epoch [96], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "97 tensor(1.0914, grad_fn=<NllLossBackward>)\n",
      "Epoch [97], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "98 tensor(1.0914, grad_fn=<NllLossBackward>)\n",
      "Epoch [98], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "99 tensor(1.0913, grad_fn=<NllLossBackward>)\n",
      "Epoch [99], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "100 tensor(1.0913, grad_fn=<NllLossBackward>)\n",
      "Epoch [100], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "101 tensor(1.0913, grad_fn=<NllLossBackward>)\n",
      "Epoch [101], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "102 tensor(1.0913, grad_fn=<NllLossBackward>)\n",
      "Epoch [102], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "103 tensor(1.0913, grad_fn=<NllLossBackward>)\n",
      "Epoch [103], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "104 tensor(1.0913, grad_fn=<NllLossBackward>)\n",
      "Epoch [104], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "105 tensor(1.0913, grad_fn=<NllLossBackward>)\n",
      "Epoch [105], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "106 tensor(1.0913, grad_fn=<NllLossBackward>)\n",
      "Epoch [106], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "107 tensor(1.0913, grad_fn=<NllLossBackward>)\n",
      "Epoch [107], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "108 tensor(1.0913, grad_fn=<NllLossBackward>)\n",
      "Epoch [108], val_loss: 1.1119, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "109 tensor(1.0913, grad_fn=<NllLossBackward>)\n",
      "Epoch [109], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "110 tensor(1.0913, grad_fn=<NllLossBackward>)\n",
      "Epoch [110], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "111 tensor(1.0913, grad_fn=<NllLossBackward>)\n",
      "Epoch [111], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "112 tensor(1.0913, grad_fn=<NllLossBackward>)\n",
      "Epoch [112], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "113 tensor(1.0913, grad_fn=<NllLossBackward>)\n",
      "Epoch [113], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "114 tensor(1.0913, grad_fn=<NllLossBackward>)\n",
      "Epoch [114], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "115 tensor(1.0913, grad_fn=<NllLossBackward>)\n",
      "Epoch [115], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "116 tensor(1.0912, grad_fn=<NllLossBackward>)\n",
      "Epoch [116], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "117 tensor(1.0912, grad_fn=<NllLossBackward>)\n",
      "Epoch [117], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "118 tensor(1.0912, grad_fn=<NllLossBackward>)\n",
      "Epoch [118], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "119 tensor(1.0912, grad_fn=<NllLossBackward>)\n",
      "Epoch [119], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "120 tensor(1.0912, grad_fn=<NllLossBackward>)\n",
      "Epoch [120], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "121 tensor(1.0912, grad_fn=<NllLossBackward>)\n",
      "Epoch [121], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "122 tensor(1.0912, grad_fn=<NllLossBackward>)\n",
      "Epoch [122], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "123 tensor(1.0912, grad_fn=<NllLossBackward>)\n",
      "Epoch [123], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "124 tensor(1.0912, grad_fn=<NllLossBackward>)\n",
      "Epoch [124], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "125 tensor(1.0912, grad_fn=<NllLossBackward>)\n",
      "Epoch [125], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "126 tensor(1.0912, grad_fn=<NllLossBackward>)\n",
      "Epoch [126], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "127 tensor(1.0912, grad_fn=<NllLossBackward>)\n",
      "Epoch [127], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "128 tensor(1.0912, grad_fn=<NllLossBackward>)\n",
      "Epoch [128], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "129 tensor(1.0912, grad_fn=<NllLossBackward>)\n",
      "Epoch [129], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "130 tensor(1.0912, grad_fn=<NllLossBackward>)\n",
      "Epoch [130], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "131 tensor(1.0911, grad_fn=<NllLossBackward>)\n",
      "Epoch [131], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "132 tensor(1.0911, grad_fn=<NllLossBackward>)\n",
      "Epoch [132], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "133 tensor(1.0911, grad_fn=<NllLossBackward>)\n",
      "Epoch [133], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "134 tensor(1.0911, grad_fn=<NllLossBackward>)\n",
      "Epoch [134], val_loss: 1.1118, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "135 tensor(1.0911, grad_fn=<NllLossBackward>)\n",
      "Epoch [135], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "136 tensor(1.0911, grad_fn=<NllLossBackward>)\n",
      "Epoch [136], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "137 tensor(1.0911, grad_fn=<NllLossBackward>)\n",
      "Epoch [137], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "138 tensor(1.0911, grad_fn=<NllLossBackward>)\n",
      "Epoch [138], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "139 tensor(1.0911, grad_fn=<NllLossBackward>)\n",
      "Epoch [139], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "140 tensor(1.0911, grad_fn=<NllLossBackward>)\n",
      "Epoch [140], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "141 tensor(1.0911, grad_fn=<NllLossBackward>)\n",
      "Epoch [141], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "142 tensor(1.0911, grad_fn=<NllLossBackward>)\n",
      "Epoch [142], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "143 tensor(1.0911, grad_fn=<NllLossBackward>)\n",
      "Epoch [143], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "144 tensor(1.0911, grad_fn=<NllLossBackward>)\n",
      "Epoch [144], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "145 tensor(1.0911, grad_fn=<NllLossBackward>)\n",
      "Epoch [145], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "146 tensor(1.0911, grad_fn=<NllLossBackward>)\n",
      "Epoch [146], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "147 tensor(1.0911, grad_fn=<NllLossBackward>)\n",
      "Epoch [147], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "148 tensor(1.0910, grad_fn=<NllLossBackward>)\n",
      "Epoch [148], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "149 tensor(1.0910, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [149], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "150 tensor(1.0910, grad_fn=<NllLossBackward>)\n",
      "Epoch [150], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "151 tensor(1.0910, grad_fn=<NllLossBackward>)\n",
      "Epoch [151], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "152 tensor(1.0910, grad_fn=<NllLossBackward>)\n",
      "Epoch [152], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "153 tensor(1.0910, grad_fn=<NllLossBackward>)\n",
      "Epoch [153], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "154 tensor(1.0910, grad_fn=<NllLossBackward>)\n",
      "Epoch [154], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "155 tensor(1.0910, grad_fn=<NllLossBackward>)\n",
      "Epoch [155], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "156 tensor(1.0910, grad_fn=<NllLossBackward>)\n",
      "Epoch [156], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "157 tensor(1.0910, grad_fn=<NllLossBackward>)\n",
      "Epoch [157], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "158 tensor(1.0910, grad_fn=<NllLossBackward>)\n",
      "Epoch [158], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "159 tensor(1.0910, grad_fn=<NllLossBackward>)\n",
      "Epoch [159], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "160 tensor(1.0910, grad_fn=<NllLossBackward>)\n",
      "Epoch [160], val_loss: 1.1117, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "161 tensor(1.0910, grad_fn=<NllLossBackward>)\n",
      "Epoch [161], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "162 tensor(1.0910, grad_fn=<NllLossBackward>)\n",
      "Epoch [162], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "163 tensor(1.0910, grad_fn=<NllLossBackward>)\n",
      "Epoch [163], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "164 tensor(1.0910, grad_fn=<NllLossBackward>)\n",
      "Epoch [164], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "165 tensor(1.0909, grad_fn=<NllLossBackward>)\n",
      "Epoch [165], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "166 tensor(1.0909, grad_fn=<NllLossBackward>)\n",
      "Epoch [166], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "167 tensor(1.0909, grad_fn=<NllLossBackward>)\n",
      "Epoch [167], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "168 tensor(1.0909, grad_fn=<NllLossBackward>)\n",
      "Epoch [168], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "169 tensor(1.0909, grad_fn=<NllLossBackward>)\n",
      "Epoch [169], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "170 tensor(1.0909, grad_fn=<NllLossBackward>)\n",
      "Epoch [170], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "171 tensor(1.0909, grad_fn=<NllLossBackward>)\n",
      "Epoch [171], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "172 tensor(1.0909, grad_fn=<NllLossBackward>)\n",
      "Epoch [172], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "173 tensor(1.0909, grad_fn=<NllLossBackward>)\n",
      "Epoch [173], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "174 tensor(1.0909, grad_fn=<NllLossBackward>)\n",
      "Epoch [174], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "175 tensor(1.0909, grad_fn=<NllLossBackward>)\n",
      "Epoch [175], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "176 tensor(1.0909, grad_fn=<NllLossBackward>)\n",
      "Epoch [176], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "177 tensor(1.0909, grad_fn=<NllLossBackward>)\n",
      "Epoch [177], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "178 tensor(1.0909, grad_fn=<NllLossBackward>)\n",
      "Epoch [178], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "179 tensor(1.0909, grad_fn=<NllLossBackward>)\n",
      "Epoch [179], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "180 tensor(1.0909, grad_fn=<NllLossBackward>)\n",
      "Epoch [180], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "181 tensor(1.0909, grad_fn=<NllLossBackward>)\n",
      "Epoch [181], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "182 tensor(1.0908, grad_fn=<NllLossBackward>)\n",
      "Epoch [182], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "183 tensor(1.0908, grad_fn=<NllLossBackward>)\n",
      "Epoch [183], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "184 tensor(1.0908, grad_fn=<NllLossBackward>)\n",
      "Epoch [184], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "185 tensor(1.0908, grad_fn=<NllLossBackward>)\n",
      "Epoch [185], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "186 tensor(1.0908, grad_fn=<NllLossBackward>)\n",
      "Epoch [186], val_loss: 1.1116, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "187 tensor(1.0908, grad_fn=<NllLossBackward>)\n",
      "Epoch [187], val_loss: 1.1115, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "188 tensor(1.0908, grad_fn=<NllLossBackward>)\n",
      "Epoch [188], val_loss: 1.1115, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "189 tensor(1.0908, grad_fn=<NllLossBackward>)\n",
      "Epoch [189], val_loss: 1.1115, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "190 tensor(1.0908, grad_fn=<NllLossBackward>)\n",
      "Epoch [190], val_loss: 1.1115, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "191 tensor(1.0908, grad_fn=<NllLossBackward>)\n",
      "Epoch [191], val_loss: 1.1115, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "192 tensor(1.0908, grad_fn=<NllLossBackward>)\n",
      "Epoch [192], val_loss: 1.1115, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "193 tensor(1.0908, grad_fn=<NllLossBackward>)\n",
      "Epoch [193], val_loss: 1.1115, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "194 tensor(1.0908, grad_fn=<NllLossBackward>)\n",
      "Epoch [194], val_loss: 1.1115, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "195 tensor(1.0908, grad_fn=<NllLossBackward>)\n",
      "Epoch [195], val_loss: 1.1115, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "196 tensor(1.0908, grad_fn=<NllLossBackward>)\n",
      "Epoch [196], val_loss: 1.1115, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "197 tensor(1.0907, grad_fn=<NllLossBackward>)\n",
      "Epoch [197], val_loss: 1.1115, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "198 tensor(1.0907, grad_fn=<NllLossBackward>)\n",
      "Epoch [198], val_loss: 1.1115, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n",
      "199 tensor(1.0907, grad_fn=<NllLossBackward>)\n",
      "Epoch [199], val_loss: 1.1115, val_acc: 0.3117, val_top_1: 0.3117, val_top_5: 1.0000\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(20)\n",
    "model = PIMANet()\n",
    "model.parameters\n",
    "if torch.cuda.is_available():\n",
    "    model=model.cuda()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "print(summary(model, input_size=(1, 8), batch_size=-1))\n",
    "for nm,params in model.named_parameters():\n",
    "    print(nm)\n",
    "    print(params.data)\n",
    "    \n",
    "evaluate(model, X_test, y_test)        \n",
    "\n",
    "\n",
    "epochs=200\n",
    "lr=0.00001\n",
    "model_state_path=\"model_state/mod_CNN.pt\"\n",
    "history2 = fit(epochs, lr, model, X_train, y_train, X_test, y_test , model_state_path=model_state_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.load_state_dict(torch.load(model_state_path))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_state_path,map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Result after training is  {'val_loss': 1.1122654676437378, 'val_acc': 0.31168830394744873, 'val_top_1': 0.31168830394744873, 'val_top_5': 1.0}\n"
     ]
    }
   ],
   "source": [
    "res = evaluate(model, X_test, y_test)\n",
    "print(\"Best Result after training is \",res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight\n",
      "tensor([[ 0.0435, -0.2281,  0.2225, -0.1205, -0.1896,  0.2002,  0.2506, -0.2820]])\n",
      "fc1.bias\n",
      "tensor([-0.2208])\n",
      "fc2.weight\n",
      "tensor([[ 0.8620],\n",
      "        [-0.8201]])\n",
      "fc2.bias\n",
      "tensor([-0.3688,  0.8846])\n"
     ]
    }
   ],
   "source": [
    "for nm,params in model.named_parameters():\n",
    "    print(nm)\n",
    "    print(params.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prune_kernel",
   "language": "python",
   "name": "prune_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
